DOCKER SWARM:
CLUSTER 
NODES
CONTAINER
APP

C: CLUSTER
N: NODE
P: POD
C: CONTAINER
A: APPLICATION

NOTE: k8s dont communicate with containers.
it communicate with pods.

COMPONENTS:
MASTER NODE:
1. API SERVER: its for communicating with cluster, it takes command executes and gives output.
2. ETCD: its a DB od cluster, all the cluster info will store here.
3. SCHEDULER: schedules pods on worker nodes, based on hardware resources.
4. CONTROLLER: used to control the k8s objects.
1. cloud controllers
2. kube controllers

WORKER NODE:
KUBELET: its an agent used to communicate with master.
KUBEPROXY: it deals with nlw.
POD: its a group of containers.


There are multiple ways to setup kubernetes cluster.

1.SELF MANAGER K8'S CLUSTER
a.mini kube (single node cluster)
b.kubeadm(multi node cluster)
c. KOPS

2. CLOUD MANAGED K8'S CLUSTER
a. AWS EKS
b.AZURE AKS
c.GCP GKS
d.IBM IKE

MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine.
It is a platform Independent.

NOTE: But we dont implement this in real-time

REQUIRMENTS:
2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.



PODS:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE: 
kubectl run pod1 --image rahamshaik/paytmtrain:latest
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

Declarative:
vim abc.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

kubectl create -f abc.yml
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

DRAWBACK:
if we delete the pod we cant retrive.
all the load will be handled by single pod.


REPLICA SET:
it will create same pod of multiple replicas.
if we delete one pod it will create automatically.
we can distribute the load also.


LABLE: assing to a pod for identification. to work with them as single unit.
SELECTOR: used to identify the pod with same label.

kubectl api-resources

REPLICASET:

apiVersion: apps/v1
kind: ReplicaSet
metadata:
  labels:
    app: swiggy
  name: swiggy-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: nginx

kubectl create -f abc.yml

kubectl get rs
kubectl get rs -o wide
kubectl describe rs swiggy-rs
kubectl delete rs swiggy-rs
kubectl edit rs/swiggy-rs



SCALING: 

SCALE-IN: Increasing the count of pods
kubectl scale rs/swiggy-rs --replicas=10

SCALE-OUT: Decreasing the count of pods
kubectl scale rs/swiggy-rs --replicas=5

SCALING FOLLOWS LIFO PATTERN:
LIFO: LAST IN FIRST OUT
the pod which is created will be deleted first automatically when we scale out.


DEPLOYMENT:
it will do all operations link RS.
it will do roll back which cannot be done in rs.

rs -- > pods
deployment -- > rs -- > pods

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-rs
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: nginx

kubectl get deploy
kubectl get deploy -o wide
kubectl describe deploy swiggy-rs
kubectl edit deploy/swiggy-rs
kubectl delete deploy swiggy-r
kubectl delete po --all


KUBECOLOR:

wget https://github.com/hidetatz/kubecolor/releases/download/v0.0.25/kubecolor_0.0.25_Linux_x86_64.tar.gz
tar -zxvf kubecolor_0.0.25_Linux_x86_64.tar.gz
./kubecolor
chmod +x kubecolor
mv kubecolor /usr/local/bin/
kubecolor get po

There are multiple ways to setup kubernetes cluster.

1.SELF MANAGER K8'S CLUSTER
a.mini kube (single node cluster)
b.kubeadm(multi node cluster)
c. KOPS

2. CLOUD MANAGED K8'S CLUSTER
a. AWS EKS
b.AZURE AKS
c.GCP GKS
d.IBM IKE

MINIKUBE:
It is a tool used to setup single node cluster on K8's. 
It contains API Servers, ETDC database and container runtime
It is used for development, testing, and experimentation purposes on local. 
Here Master and worker runs on same machine.
It is a platform Independent.

NOTE: But we dont implement this in real-time

REQUIRMENTS:
2 CPUs or more
2GB of free memory
20GB of free disk space
Internet connection
Container or virtual machine manager, such as: Docker.

SETUP:
sudo apt update -y
sudo apt upgrade -y
sudo apt install curl wget apt-transport-https -y
sudo curl -fsSL https://get.docker.com -o get-docker.sh
sudo sh get-docker.sh
sudo curl -LO https://storage.googleapis.com/minikube/releases/latest/minikube-linux-amd64
sudo mv minikube-linux-amd64 /usr/local/bin/minikube
sudo chmod +x /usr/local/bin/minikube
sudo minikube version
sudo curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
sudo curl -LO "https://dl.k8s.io/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl.sha256"
sudo echo "$(cat kubectl.sha256) kubectl" | sha256sum --check
sudo install -o root -g root -m 0755 kubectl /usr/local/bin/kubectl
sudo minikube start --driver=docker --force

PODS:
It is a smallest unit of deployment in K8's.
It is a group of containers.
Pods are ephemeral (short living objects)
Mostly we can use single container inside a pod but if we required, we can create multiple containers inside a same pod.
when we create a pod, containers inside pods can share the same network namespace, and can share the same storage volumes .
While creating pod, we must specify the image, along with any necessary configuration and resource limits.
K8's cannot communicate with containers, they can communicate with only pods.
 We can create this pod in two ways, 
1. Imperative(command) 
2. Declarative (Manifest file)


IMPERATIVE: 
kubectl run pod1 --image rahamshaik/paytmtrain:latest
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

Declarative:
vim abc.yml

apiVersion: v1
kind: Pod
metadata:
   name: pod1
spec:
  containers:
    - image: nginx
      name: cont1

kubectl create -f abc.yml
kubectl get po/pod/pods
kubectl get po/pod/pods -o wide
kubectl describe pod pod1
kubectl delete pod pod1

KOPS:
INFRASTRUCTURE: Resources used to run our application on cloud.
EX: Ec2, VPC, ALB, -------------


Minikube -- > single node cluster
All the pods on single node 
kOps, also known as Kubernetes operations.
it is an open-source tool that helps you create, destroy, upgrade, and maintain a highly available, production-grade Kubernetes cluster. 
Depending on the requirement, kOps can also provide cloud infrastructure.
kOps is mostly used in deploying AWS and GCE Kubernetes clusters. 
But officially, the tool only supports AWS. Support for other cloud providers (such as DigitalOcean, GCP, and OpenStack) are in the beta stage.


ADVANTAGES:
•	Automates the provisioning of AWS and GCE Kubernetes clusters
•	Deploys highly available Kubernetes masters
•	Supports rolling cluster updates
•	Autocompletion of commands in the command line
•	Generates Terraform and CloudFormation configurations
•	Manages cluster add-ons.
•	Supports state-sync model for dry-runs and automatic idempotency
•	Creates instance groups to support heterogeneous clusters

ALTERNATIVES:
Amazon EKS , MINIKUBE, KUBEADM, RANCHER, TERRAFORM.


STEP-1: GIVING PERMISSIONS
IAM -- > USER -- > CREATE USER -- > NAME: KOPS -- > Attach Polocies Directly -- > AdministratorAccess -- > NEXT -- > CREATE USER
USER -- > SECURTITY CREDENTIALS -- > CREATE ACCESS KEYS -- > CLI -- > CHECKBOX -- >  CREATE ACCESS KEYS -- > DOWNLOAD 

aws configure

SETP-2: INSTALL KUBECTL AND KOPS

curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
wget https://github.com/kubernetes/kops/releases/download/v1.25.0/kops-linux-amd64
chmod +x kops-linux-amd64 kubectl
mv kubectl /usr/local/bin/kubectl
mv kops-linux-amd64 /usr/local/bin/kops

vim .bashrc
export PATH=$PATH:/usr/local/bin/  -- > save and exit
source .bashrc

SETP-3: CREATEING BUCKET 
aws s3api create-bucket --bucket devopsbyraham007.k8s.local --region us-east-1
aws s3api put-bucket-versioning --bucket cloudanddevopsbyraham007.k8s.local --region us-east-1 --versioning-configuration Status=Enabled
export KOPS_STATE_STORE=s3://cloudanddevopsbyraham007.k8s.local

SETP-4: CREATING THE CLUSTER
kops create cluster --name rahams.k8s.local --zones us-east-1a --master-count=1 --master-size t2.medium --node-count=2 --node-size t2.micro
kops update cluster --name rahams.k8s.local --yes --admin


Suggestions:
 * list clusters with: kops get cluster
 * edit this cluster with: kops edit cluster rahams.k8s.local
 * edit your node instance group: kops edit ig --name=rahams.k8s.local nodes-us-east-1a
 * edit your master instance group: kops edit ig --name=rahams.k8s.local master-us-east-1a


ADMIN ACTIVITIES:
To scale the worker nodes:
kops edit ig --name=rahams.k8s.local nodes-us-east-1a
kops update cluster --name rahams.k8s.local --yes --admin 
kops rolling-update cluster --yes

kubectl describe node node_id
kops delete cluster --name rahams.k8s.local --yes

NAMESPACES:

NAMESPACE: It is used to divide the cluster to multiple teams on real time.
it is used to isolate the env.

CLUSTER: HOUSE
NAMESPACES: ROOM

Each namespace is isolated.
if your are room-1 are you able to see room-2.
we cant access the objects from one namespace to another namespace.


TYPES:

default           : Is the default namespace, all objects will create here only
kube-node-lease   : it will store object which is taken from one namespace to another.
kube-public	  : all the public objects will store here.      
kube-system 	  : default k8s will create some objects, those are storing on this ns.

kubectl get pod -n kube-system	: to list all pods in kube-system namespace
kubectl get pod -n default	: to list all pods in default namespace
kubectl get pod -n kube-public	: to list all pods in kube-public namespace
kubectl get po -A		: to list all pods in all namespaces

kubectl create ns dev	: to create namespace
kubectl config set-context --current --namespace=dev : to switch to the namespace
kubectl config view --minify | grep namespace : to see current namespace
kubectl delete ns dev	: to delete namespace
kubectl delete pod --all: to delete all pods


NOTE: By deleting  the ns all objects also gets deleted.
in real time we use rbac concept to restrict the access from one namespace to another.
so users cant access/delete ns, because of the restriction we provide.
we create roles and rolebind for the users.


SERVICE: It is used to expose the application in k8s.

TYPES:
1. CLUSTERIP: It will work inside the cluster.
it will not expose to outer world.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/moviespaytm:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: sv1
spec:
  type: ClusterIP
  selector:
    app: swiggy
  ports:
    - port: 80

DRAWBACK:
We cannot use app outside.

2. NODEPORT: It will expose our application in a particular port.
Range: 30000 - 32767 (in sg we need to give all traffic)

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: NodePort
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80
      nodePort: 31111

NOTE: UPDATE THE SG (REMOVE OLD TRAFFIC AND GIVE ALL TRAFFIC & SSH)
DRAWBACK:
PORT RESTRICTION.

3. LOADBALACER: It will expose our app and distribute load blw pods.

apiVersion: apps/v1
kind: Deployment
metadata:
  labels:
    app: swiggy
  name: swiggy-deploy
spec:
  replicas: 3
  selector:
    matchLabels:
      app: swiggy
  template:
    metadata:
      labels:
        app: swiggy
    spec:
      containers:
      - name: cont1
        image: rahamshaik/trainservice:latest
        ports:
          - containerPort: 80
---
apiVersion: v1
kind: Service
metadata:
  name: abc
spec:
  type: LoadBalancer
  selector:
    app: swiggy
  ports:
    - port: 80
      targetPort: 80